{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from elasticsearch7 import Elasticsearch\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from time import time, sleep\n",
    "from tqdm import tqdm\n",
    "from queue import PriorityQueue\n",
    "import os\n",
    "\n",
    "es = Elasticsearch(\"http://localhost:9200/\")\n",
    "print(es.ping())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "\n",
    "def safe_request(url, max_retries=3, backoff_factor=0.5):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = session.get(url, timeout=10, allow_redirects=True)\n",
    "            response.raise_for_status()  # Raises a HTTPError for non-200 responses\n",
    "            return response\n",
    "        except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError, requests.exceptions.Timeout) as e:\n",
    "            if e.response.status_code == 404:\n",
    "                return None  # Treat 404 as a special case to handle later\n",
    "            sleep(backoff_factor * (2 ** attempt))\n",
    "        except requests.exceptions.RequestException:\n",
    "            sleep(backoff_factor * (2 ** attempt))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonicalize_url_map = {}\n",
    "def canonicalize_url(url, base_url=None):\n",
    "    if url in canonicalize_url_map:\n",
    "        return canonicalize_url_map[url]\n",
    "    url = url.lower()\n",
    "    parts = url.split(\"://\", 1)\n",
    "    if len(parts) == 2:\n",
    "        protocol, rest = parts\n",
    "    else:\n",
    "        protocol, rest = \"\", parts[0]\n",
    " \n",
    "    parts = rest.split(\"/\", 1)\n",
    "    host = parts[0]\n",
    "    path = \"/\"\n",
    "    if len(parts) == 2:\n",
    "        if parts[1] == \"\":\n",
    "            host = parts[0].split(\"#\")[0]\n",
    "        path = \"/\" + parts[1]\n",
    " \n",
    "    host_parts = host.split(\":\")\n",
    "    hostname = host_parts[0]\n",
    "    if len(host_parts) == 2 and host_parts[1].isdigit():\n",
    "        port = int(host_parts[1])\n",
    "    else:\n",
    "        port = None\n",
    " \n",
    "    if (protocol == \"http\" and port == 80) or (protocol == \"https\" and port == 443):\n",
    "        host = hostname\n",
    "    else:\n",
    "        if port:\n",
    "            host = \":\".join([hostname, str(port)])\n",
    "        else:\n",
    "            host = hostname\n",
    " \n",
    "    if base_url and not protocol:\n",
    "        base_parts = base_url.split(\"://\", 1)\n",
    "        base_protocol = base_parts[0]\n",
    "        base_rest = base_parts[1] if len(base_parts) == 2 else \"\"\n",
    "        base_host = base_rest.split(\"/\", 1)[0]\n",
    "        url = urljoin(base_protocol + \"://\" + base_host, url)\n",
    "        return canonicalize_url(url)\n",
    " \n",
    "    path = path.split(\"#\")[0]    \n",
    "    path = \"/\"+\"/\".join([part for part in path.split(\"/\") if part])\n",
    "    canonicalized_url = protocol + \"://\" + host + path\n",
    "    if not canonicalized_url.startswith('http') or 'javascript' in canonicalized_url or 'pdf' in canonicalized_url or 'svg' in canonicalized_url or 'jpg' in canonicalized_url or 'png' in canonicalized_url or 'gif' in canonicalized_url or 'jpeg' in canonicalized_url:\n",
    "        canonicalize_url_map[url] = None    \n",
    "        return None\n",
    "    canonicalize_url_map[url] = canonicalized_url\n",
    "    return canonicalized_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_html(url, retries=3, backoff_factor=0.5):\n",
    "    response = safe_request(url, max_retries=retries, backoff_factor=backoff_factor)\n",
    "    if response and 'text/html' in response.headers.get('Content-Type', ''):\n",
    "        final_url = response.url  # Capture the final URL after redirections\n",
    "        return {\n",
    "            'html_content': response.text,\n",
    "            'final_url': final_url\n",
    "        }\n",
    "    else:\n",
    "        if response:\n",
    "            print(f\"Non-200 status code received: {response.status_code} for URL: {url}\")\n",
    "        return None\n",
    "    \n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  \n",
    "    # text = re.sub(r'[^\\w\\s]', '', text) \n",
    "    text = re.sub(r'[^\\w\\s\\-\\'’]', '', text)\n",
    "    text = re.sub(r'[»«|]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def parse_html(html_content, url):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "    title = soup.title.string if soup.title else ''\n",
    "    text = ' '.join(soup.stripped_strings)\n",
    "    text = preprocess_text(text)\n",
    "\n",
    "    # Attempt to extract terms and conditions\n",
    "    terms_text = extract_terms_and_conditions(soup)\n",
    "\n",
    "    return links, title, text, terms_text\n",
    "\n",
    "def extract_terms_and_conditions(soup):\n",
    "    # Terms and conditions are often located under specific headings or links\n",
    "    terms_headings = ['legal', 'privacy policy']\n",
    "    \n",
    "    terms_text = \"\"\n",
    "    for heading in terms_headings:\n",
    "        for element in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'a', 'div'], string=re.compile(heading, re.I), limit=1):\n",
    "            \n",
    "            # Attempt to follow link if 'a' tag is found\n",
    "            if element.name == 'a' and element.has_attr('href'):\n",
    "                terms_url = element['href']\n",
    "                if not terms_url.startswith('http'):\n",
    "                    # Handle relative URLs\n",
    "                    terms_url = urlparse.urljoin(url, terms_url)\n",
    "                terms_content = fetch_html(terms_url)\n",
    "                \n",
    "                if terms_content:\n",
    "                    terms_soup = BeautifulSoup(terms_content['html_content'], 'html.parser')\n",
    "                    terms_text += ' '.join(terms_soup.stripped_strings)\n",
    "            else:\n",
    "                # Extract text from a section if it's not a link\n",
    "                next_node = element.find_next_sibling()\n",
    "                while next_node and next_node.name not in ['h1', 'h2', 'h3', 'h4', 'h5', 'div']:\n",
    "                    terms_text += next_node.get_text(strip=True) + \" \"\n",
    "                    next_node = next_node.find_next_sibling()\n",
    "\n",
    "    return terms_text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Placeholder for your text preprocessing logic\n",
    "    return text.strip()\n",
    "\n",
    "def store_document(url, text, raw_html, outlinks, title):\n",
    "    document = {\n",
    "        'url': url,\n",
    "        'text': text,\n",
    "        'raw_html': raw_html,\n",
    "        'outlinks': outlinks,\n",
    "        'title': title,\n",
    "        'timestamp': datetime.now()\n",
    "    }\n",
    "    es.index(index=\"web-crawl\", document=document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "robots_cache = {}\n",
    "\n",
    "def check_robots_txt(url, retries=2, backoff_factor=0.5):\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "    \n",
    "    if domain in robots_cache:\n",
    "        rp = robots_cache[domain]\n",
    "    else:\n",
    "        robots_url = f\"{domain}/robots.txt\"\n",
    "        rp = RobotFileParser()\n",
    "        response = safe_request(robots_url, max_retries=retries, backoff_factor=backoff_factor)\n",
    "        \n",
    "        if response is None:  # Handle 404 for robots.txt by allowing crawling\n",
    "            print(f\"robots.txt not found at {robots_url}, assuming full access allowed.\")\n",
    "            rp.allow_all = True\n",
    "        elif response and response.status_code == 200:\n",
    "            rp.parse(response.text.splitlines())\n",
    "        else:\n",
    "            print(f\"Unable to fetch robots.txt for {url}, proceeding with caution.\")\n",
    "            rp.allow_all = True  # Default behavior when fetch fails\n",
    "        \n",
    "        robots_cache[domain] = rp  # Cache the parsed rules or the decision to allow all\n",
    "    \n",
    "    # Check if the URL is allowed by the cached robots.txt rules\n",
    "    return rp.can_fetch('*', url) if rp else True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_request_time = {}\n",
    "\n",
    "def throttle_request(domain):\n",
    "    now = time()\n",
    "    if domain in last_request_time:\n",
    "        elapsed = now - last_request_time[domain]\n",
    "        if elapsed < 1:\n",
    "            sleep(1 - elapsed)\n",
    "    last_request_time[domain] = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(url, title, text, keywords, base_score=1):\n",
    "    keyword_score = sum([title.count(keyword) + text.count(keyword) for keyword in keywords])\n",
    "    # Assume a simple scoring model where each keyword match increases the score\n",
    "    return base_score + keyword_score\n",
    "\n",
    "def count_keyword_matches(text, keywords):\n",
    "    total_matches = 0\n",
    "    # Prepare the text for case-insensitive matching\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        # Use a regular expression to find whole word matches, case-insensitive\n",
    "        pattern = r'\\b' + re.escape(keyword.lower()) + r'\\b'\n",
    "        matches = re.findall(pattern, text_lower)\n",
    "        total_matches += len(matches)\n",
    "    \n",
    "    return total_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Priority Queue for the Frontier\n",
    "frontier = PriorityQueue()\n",
    "visited_urls = set()\n",
    "link_graph = {}\n",
    "\n",
    "def add_to_frontier(url, title, keywords, base_score=1):    \n",
    "    # Calculate score based on keyword matches in the URL and title\n",
    "    keyword_score = sum([title.count(keyword) + url.count(keyword) for keyword in keywords])\n",
    "    score = base_score + keyword_score\n",
    "    \n",
    "    # Use negative score as priority to ensure that PriorityQueue treats higher scores as higher priority\n",
    "    priority = -score\n",
    "    frontier.put((priority, url))\n",
    "\n",
    "def update_link_graph(url, inlinks=None, outlinks=None, wave_number=0, text=\"\", keywords=[]):\n",
    "    inlink_count = len(inlinks) if inlinks else 0\n",
    "    keyword_matches = count_keyword_matches(text, keywords)\n",
    "    \n",
    "    if url not in link_graph:\n",
    "        link_graph[url] = {\n",
    "            \"inlinks\": set(inlinks) if inlinks else set(),\n",
    "            \"outlinks\": set(outlinks) if outlinks else set(),\n",
    "            \"wave_number\": wave_number,\n",
    "            \"score\": calculate_score(inlink_count, wave_number, keyword_matches)\n",
    "        }\n",
    "    else:\n",
    "        existing_entry = link_graph[url]\n",
    "        if inlinks:\n",
    "            existing_entry[\"inlinks\"].update(inlinks)\n",
    "        if outlinks:\n",
    "            existing_entry[\"outlinks\"].update(outlinks)\n",
    "        # Recalculate the score with updated values\n",
    "        existing_entry[\"score\"] = calculate_score(len(existing_entry[\"inlinks\"]), wave_number, keyword_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_document(url, title, text,terms):\n",
    "    return f\"<DOC>\\n<DOCNO>{url}</DOCNO>\\n<TERMS>{terms}</TERMS>\\n<HEAD>{title}</HEAD></DOC>\\n\"\n",
    "\n",
    "def write_document_single(file_path, document):\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords = ['american', 'revolution', 'the-war-at-sea', 'war', 'sea', 'american-revolution', 'battle', 'history', 'navy']\n",
    "keywords = [\n",
    "    \"Privacy\",\n",
    "    \"Legal\",\n",
    "    \"Policy\",\n",
    "    \"Compliance\",\n",
    "    \"Security\",\n",
    "    \"Copyright\",\n",
    "    \"Disclaimer\",\n",
    "    \"Ethics\",\n",
    "    \"Regulations\",\n",
    "    \"Guidelines\",\n",
    "    \"Safety\",\n",
    "    \"Data\",\n",
    "    \"Usage\",\n",
    "    \"Agreement\",\n",
    "    \"License\",\n",
    "    \"Conditions\",\n",
    "    \"Cookies\",\n",
    "    \"GDPR\",  # European privacy law\n",
    "    \"HIPAA\"  # US healthcare compliance\n",
    "]\n",
    "def initialize_frontier_with_seeds(seed_urls, keywords):\n",
    "    for url in seed_urls:\n",
    "        add_to_frontier(url, \"\", keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "def store_link_graph(link_graph, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(link_graph, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_graph = {}\n",
    "visited_urls = set()\n",
    "\n",
    "def main_crawl(start_urls, target_count, keywords):\n",
    "    doc_counter = 343  # Initialize document counter\n",
    "    \n",
    "\n",
    "    initialize_frontier_with_seeds(start_urls, keywords)\n",
    "    print(\"Initialized frontier with seed URLs.\")\n",
    "\n",
    "    with tqdm(total=target_count) as pbar:\n",
    "        while not frontier.empty() and len(visited_urls) < target_count:\n",
    "            try:\n",
    "                priority, current_url = frontier.get(False)  # Fetch the current URL from the frontier without blocking\n",
    "                \n",
    "                if current_url in visited_urls:  # Skip if the URL has already been crawled\n",
    "                    continue\n",
    "\n",
    "                if not check_robots_txt(current_url):  # Proceed only if robots.txt allows crawling\n",
    "                    continue\n",
    "\n",
    "                throttle_request(urlparse(current_url).netloc)  # Respect crawl delay\n",
    "                content = fetch_html(current_url)  # Fetch content and outlinks\n",
    "                \n",
    "                if content and content['html_content']:\n",
    "                    visited_urls.add(current_url)\n",
    "                    links, title, text,terms = parse_html(content['html_content'], content['final_url'])\n",
    "\n",
    "                    # Update link_graph with inlinks and outlinks, ensuring they are lists\n",
    "                    if current_url not in link_graph:\n",
    "                        link_graph[current_url] = {'inlinks': [], 'outlinks': [link for link in links]}\n",
    "                    else:\n",
    "                        link_graph[current_url]['outlinks'].extend([link for link in links if link not in link_graph[current_url]['outlinks']])\n",
    "\n",
    "                    for link in links:\n",
    "                        canonical_link = canonicalize_url(link, content['final_url'])\n",
    "                        if canonical_link and canonical_link not in visited_urls:\n",
    "                            add_to_frontier(canonical_link, title, keywords)  # Use keyword scoring for added URLs\n",
    "                            # Update inlinks for the canonical link, ensuring it's a list\n",
    "                            if canonical_link not in link_graph:\n",
    "                                link_graph[canonical_link] = {'inlinks': [current_url], 'outlinks': []}\n",
    "                            else:\n",
    "                                if current_url not in link_graph[canonical_link]['inlinks']:\n",
    "                                    link_graph[canonical_link]['inlinks'].append(current_url)\n",
    "\n",
    "                    # Document handling\n",
    "                    \n",
    "                    if terms:\n",
    "                        doc_counter += 1\n",
    "                        doc_file_name = f\"document_{doc_counter}.txt\"\n",
    "                        doc_file_path = os.path.join(\"/Users/vikashmediboina/Projects/Aravind_scrapper/privacy\", doc_file_name)  # Ensure directory exists\n",
    "                        formatted_doc = format_document(content['final_url'], title, text,terms)\n",
    "                        write_document_single(doc_file_path, formatted_doc)\n",
    "\n",
    "                    pbar.update(1)\n",
    "                else:\n",
    "                    print(f\"No content fetched for: {current_url}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {current_url}: {e}\")\n",
    "\n",
    "            store_link_graph(link_graph, \"/Users/vikashmediboina/Projects/Aravind_scrapper/Results/graph.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call main_crawl with seed URLs, target count, and keywords\n",
    "# https://www.worldwildlife.org/threats/effects-of-climate-change\n",
    "# 'https://en.wikipedia.org/wiki/Effects_of_climate_change_on_biodiversity'\n",
    "seed_urls = ['https://policy.medium.com/medium-privacy-policy-f03bf92035c9', 'https://stackoverflow.com/legal/privacy-policy', 'https://www.facebook.com/privacy/policy/']\n",
    "main_crawl(seed_urls, 3500, keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Assuming Elasticsearch is running locally and the index name is 'myindex'\n",
    "index_name = \"privacy-policy\"\n",
    "new_cloud_id = \"c83822bf9f204bd3928fb2b3deedf98a:dXMtY2VudHJhbDEuZ2NwLmNsb3VkLmVzLmlvOjQ0MyQzNDUyNGE2NzA3ZjI0NDM0YmRkM2E0ZWI1OTBhZDgzMCQ2ODg2ZjEwNWM4YzQ0ZWE0YjNjNzk1YzRlMDkwYWJmOQ==\"\n",
    "new_auth = (\"elastic\", \"rCaLfA66eg7TNgaaqhRZd7AZ\")\n",
    "es = Elasticsearch(timeout=10000, cloud_id=new_cloud_id, http_auth=new_auth)\n",
    "\n",
    "es.ping()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 594/594 [00:01<00:00, 395.15it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Merge Index on Elasticsearch cloud\"\"\"\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from elasticsearch7 import Elasticsearch, helpers\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_files_in_dir(folder_path):\n",
    "    # gets all names of files in directory\n",
    "    file_list = os.listdir(folder_path)\n",
    "\n",
    "    # append them to list with their full paths\n",
    "    file_path_list = []\n",
    "    for file in file_list:\n",
    "        file_path_list.append(os.path.join(folder_path, file))\n",
    "\n",
    "    return file_path_list\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Function: get_data_from_text_file()\n",
    "Input: file: a single file that may contain multiple documents to be indexed\n",
    "Returns: data: a list of lists; each sub-list is a line from the file\n",
    "Does: reads each line of the file and appends it in a list to data\n",
    "\"\"\"\n",
    "def get_data_from_text_file(file):\n",
    "    # declare an empty list for the data\n",
    "    data = []\n",
    "    for line in open(file, encoding=\"ISO-8859-1\", errors='ignore'):\n",
    "        data += [str(line)]\n",
    "    return data\n",
    "\"\"\"\n",
    "Function: yield_docs()\n",
    "Input: files: a list of each file path that we want to index (each file contains one doc)\n",
    "Returns: null\n",
    "Does: For each file, get the fields that we need and do some text clean up. Check if the doc is already in the corpus.\n",
    "If it is, update the author and inlinks. If it isn't stage it to be indexed. \n",
    "\"\"\"\n",
    "def yield_docs(files):\n",
    "    unique_docs = set()\n",
    "    for count, file in enumerate(tqdm(files)):\n",
    "        \n",
    "        # retrieve data from file\n",
    "        doc = get_data_from_text_file(file)\n",
    "        doc = \"\".join(doc)\n",
    "        \n",
    "        # get doc no\n",
    "        docno_s = doc.find(\"<DOCNO>\") + len(\"<DOCNO>\") \n",
    "        docno_e = doc.find(\"</DOCNO>\")\n",
    "        docno = doc[docno_s:docno_e].strip()\n",
    "        id = docno.split(\"/\")[2]\n",
    "\n",
    "        # get title\n",
    "        title_s = doc.find(\"<HEAD>\") + len(\"<HEAD>\") \n",
    "        ttile_e = doc.find(\"</HEAD>\")\n",
    "        title = doc[title_s:ttile_e].strip()\n",
    "\n",
    "        # find text\n",
    "        text_s = doc.find(\"<TERMS>\") + len(\"<TERMS>\") \n",
    "        text_e = doc.find(\"</TERMS>\")\n",
    "        text = doc[text_s:text_e].strip()\n",
    "        text = re.sub(r'\\n+', '\\n', text).strip()\n",
    "        text = text.lower()\n",
    "        \n",
    "        # text cleaning\n",
    "        text_start_cut = text.find(\"jump to search\")\n",
    "        if text_start_cut != -1:\n",
    "            text = text[text_start_cut+len(\"jump to search\"):]\n",
    "        text_end_cut3 = text.find(\"sources[edit]\")\n",
    "        if text_end_cut3 != -1:\n",
    "            text = text[:text_end_cut3]\n",
    "        text_end_cut4 = text.find(\"this page was last edited\")\n",
    "        if text_end_cut4 != -1:\n",
    "            text = text[:text_end_cut4]\n",
    "        text_end_cut5 = text.find(\"navigation menu\")\n",
    "        if text_end_cut5 != -1:\n",
    "            text = text[:text_end_cut5]\n",
    "        text = text.replace(\"[edit]\", \" \")\n",
    "        \n",
    "        # push to es\n",
    "        if id not in unique_docs:\n",
    "            document_data = {\n",
    "                'url': docno,\n",
    "                'version': 1,\n",
    "                'terms': text,\n",
    "                'title': title,\n",
    "                'timestamp': datetime.now().isoformat()  # optional, adds a timestamp\n",
    "            }\n",
    "            response = es.index(index=index_name, document=document_data)\n",
    "            unique_docs.add(id)\n",
    "            \n",
    "file_path = '/Users/vikashmediboina/Projects/Aravind_scrapper/privacy'\n",
    "all_files = get_files_in_dir(file_path)\n",
    "print(len(all_files))\n",
    "yield_docs(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('crawleer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7f9c22d6688a4b460636b9ec4d369f8a560be8d2a235fa298866882e1582f054"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
